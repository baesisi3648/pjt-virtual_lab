# @TASK P3-R1-T1 - FastAPI Backend Server
# @SPEC TASKS.md#P3-R1-T1
# @TEST tests/test_server.py
"""FastAPI Backend Server

Virtual Lab ì—°êµ¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•˜ëŠ” REST API ì„œë²„ì…ë‹ˆë‹¤.
Streamlit í”„ë¡ íŠ¸ì—”ë“œì™€ CORSë¥¼ í†µí•´ ì—°ë™ë©ë‹ˆë‹¤.
"""
import asyncio
import json
from typing import AsyncGenerator

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel

from workflow.graph import create_workflow
from workflow.state import AgentState
from celery_app import app as celery_app
from tasks.research_task import run_research as celery_run_research, health_check


app = FastAPI(title="Virtual Lab API")

# CORS ì„¤ì • (Streamlit + Next.js ì—°ë™)
# ê°œë°œ í™˜ê²½: ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©
# í”„ë¡œë•ì…˜ í™˜ê²½: íŠ¹ì • ì˜¤ë¦¬ì§„ë§Œ í—ˆìš© ê¶Œì¥
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # Next.js dev server
        "http://localhost:8501",  # Streamlit dev server
        "*",  # ê°œë°œ í¸ì˜ë¥¼ ìœ„í•´ ëª¨ë“  ì˜¤ë¦¬ì§„ í—ˆìš©
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ResearchRequest(BaseModel):
    """ì—°êµ¬ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    topic: str
    constraints: str = ""


class ResearchResponse(BaseModel):
    """ì—°êµ¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    report: str
    messages: list[dict]
    iterations: int


class AsyncResearchRequest(BaseModel):
    """ë¹„ë™ê¸° ì—°êµ¬ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    query: str


class AsyncResearchResponse(BaseModel):
    """ë¹„ë™ê¸° ì—°êµ¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    task_id: str
    status: str
    message: str


class TaskStatusResponse(BaseModel):
    """íƒœìŠ¤í¬ ìƒíƒœ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    task_id: str
    status: str
    result: dict | None = None
    error: str | None = None


class RegenerateRequest(BaseModel):
    """ë³´ê³ ì„œ ì„¹ì…˜ ì¬ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    section: str
    feedback: str
    current_report: str = ""


class RegenerateResponse(BaseModel):
    """ë³´ê³ ì„œ ì„¹ì…˜ ì¬ìƒì„± ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    updated_report: str
    section: str
    message: str


@app.get("/health")
def health_check_endpoint():
    """í—¬ìŠ¤ì²´í¬"""
    return {"status": "ok"}


@app.post("/api/research", response_model=ResearchResponse)
def run_research(request: ResearchRequest):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

    LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•˜ê³  ì´ˆê¸° ìƒíƒœë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    Scientist -> Critic -> PI íë¦„ì„ ê±°ì³ ìµœì¢… ë³´ê³ ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì›Œí¬í”Œë¡œìš° ìƒì„±
    workflow = create_workflow()

    # ì´ˆê¸° ìƒíƒœ
    initial_state: AgentState = {
        "topic": request.topic,
        "constraints": request.constraints,
        "draft": "",
        "critique": None,
        "iteration": 0,
        "final_report": "",
        "messages": [],
    }

    # ì‹¤í–‰
    result = workflow.invoke(initial_state)

    return ResearchResponse(
        report=result["final_report"],
        messages=result["messages"],
        iterations=result["iteration"],
    )


@app.post("/api/research/async", response_model=AsyncResearchResponse)
async def submit_async_research(request: AsyncResearchRequest):
    """ë¹„ë™ê¸° ì—°êµ¬ ì‘ì—… ì œì¶œ

    ì¥ì‹œê°„ ì†Œìš”ë˜ëŠ” ì—°êµ¬ ì‘ì—…ì„ Celeryë¥¼ í†µí•´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    task_idë¥¼ ë°˜í™˜í•˜ë©°, /api/task/{task_id}ë¡œ ìƒíƒœë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        task = celery_run_research.delay(request.query)
        return AsyncResearchResponse(
            task_id=task.id,
            status="processing",
            message="Research task submitted successfully"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to submit research task: {str(e)}"
        )


@app.get("/api/task/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """íƒœìŠ¤í¬ ìƒíƒœ ì¡°íšŒ

    Celery íƒœìŠ¤í¬ì˜ í˜„ì¬ ìƒíƒœì™€ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

    ìƒíƒœ ì¢…ë¥˜:
    - PENDING: ëŒ€ê¸° ì¤‘
    - PROGRESS: ì§„í–‰ ì¤‘
    - SUCCESS: ì™„ë£Œ
    - FAILURE: ì‹¤íŒ¨
    """
    try:
        task = celery_app.AsyncResult(task_id)

        if task.state == 'PENDING':
            return TaskStatusResponse(
                task_id=task_id,
                status="pending",
                result=None
            )
        elif task.state == 'PROGRESS':
            return TaskStatusResponse(
                task_id=task_id,
                status="progress",
                result=task.info if task.info else None
            )
        elif task.state == 'SUCCESS':
            return TaskStatusResponse(
                task_id=task_id,
                status="success",
                result=task.result
            )
        elif task.state == 'FAILURE':
            return TaskStatusResponse(
                task_id=task_id,
                status="failure",
                result=None,
                error=str(task.info)
            )
        else:
            return TaskStatusResponse(
                task_id=task_id,
                status=task.state.lower(),
                result=task.info if task.info else None
            )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get task status: {str(e)}"
        )


@app.get("/api/celery/health")
async def celery_health_check():
    """Celery ì›Œì»¤ í—¬ìŠ¤ì²´í¬

    Celery ì›Œì»¤ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        # Simple task to check if worker is alive
        result = health_check.delay()
        # Wait max 5 seconds
        response = result.get(timeout=5)
        return {
            "status": "ok",
            "celery_status": response.get("status"),
            "message": response.get("message")
        }
    except Exception as e:
        return {
            "status": "error",
            "celery_status": "unavailable",
            "message": f"Celery worker not responding: {str(e)}"
        }


# P4-T2: SSE ì—”ë“œí¬ì¸íŠ¸
async def generate_research_events(topic: str, constraints: str) -> AsyncGenerator[str, None]:
    """ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ì´ë²¤íŠ¸ë¥¼ SSE í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

    Args:
        topic: ì—°êµ¬ ì£¼ì œ
        constraints: ì œì•½ ì¡°ê±´

    Yields:
        str: SSE í˜•ì‹ì˜ ì´ë²¤íŠ¸ ë¬¸ìì—´ (data: {...}\n\n)
    """
    def send_event(event_type: str, data: dict):
        """SSE ì´ë²¤íŠ¸ ì „ì†¡ í—¬í¼"""
        event_data = {
            "type": event_type,
            "timestamp": asyncio.get_event_loop().time(),
            **data
        }
        return f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"

    try:
        # ì‹œì‘ ì´ë²¤íŠ¸
        yield send_event("start", {
            "message": "ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...",
            "topic": topic
        })

        # ì›Œí¬í”Œë¡œìš° ìƒì„±
        workflow = create_workflow()

        # ì´ˆê¸° ìƒíƒœ
        initial_state: AgentState = {
            "topic": topic,
            "constraints": constraints,
            "draft": "",
            "critique": None,
            "iteration": 0,
            "final_report": "",
            "messages": [],
        }

        # Phase 1: Drafting ì‹œì‘
        yield send_event("phase", {
            "phase": "drafting",
            "agent": "scientist",
            "message": "ğŸ”¬ Scientist: ìœ„í—˜ ìš”ì†Œ ë¶„ì„ ì¤‘..."
        })

        await asyncio.sleep(0.1)  # ì´ë²¤íŠ¸ ì „ì†¡ ë³´ì¥

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ)
        # LangGraphì˜ stream() ë©”ì„œë“œëŠ” ë™ê¸° APIì´ë¯€ë¡œ asyncio.to_threadë¡œ ì‹¤í–‰
        iteration_count = 0
        final_result = None

        # ë™ê¸° streamì„ ë¹„ë™ê¸° generatorë¡œ ë³€í™˜
        for event in workflow.stream(initial_state):
            # ë…¸ë“œë³„ ì´ë²¤íŠ¸ ì „ì†¡
            for node_name, node_state in event.items():
                if node_name == "drafting":
                    yield send_event("agent", {
                        "agent": "scientist",
                        "phase": "drafting",
                        "message": "ğŸ”¬ Scientist: ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ",
                        "iteration": iteration_count + 1
                    })
                elif node_name == "critique":
                    yield send_event("agent", {
                        "agent": "critic",
                        "phase": "critique",
                        "message": "ğŸ” Critic: ì´ˆì•ˆ ê²€í†  ì¤‘...",
                        "iteration": iteration_count + 1
                    })

                    # Critique ê²°ê³¼ í™•ì¸
                    if node_state.get("critique"):
                        critique = node_state["critique"]
                        decision = critique.decision

                        if decision == "revise":
                            yield send_event("decision", {
                                "agent": "critic",
                                "decision": "revise",
                                "message": "âŒ Critic: ìˆ˜ì • í•„ìš”",
                                "feedback": critique.feedback[:100] + "..."
                            })
                        else:
                            yield send_event("decision", {
                                "agent": "critic",
                                "decision": "approve",
                                "message": "âœ… Critic: ìŠ¹ì¸"
                            })

                elif node_name == "increment":
                    iteration_count += 1
                    yield send_event("iteration", {
                        "iteration": iteration_count,
                        "message": f"ğŸ”„ ë°˜ë³µ {iteration_count}íšŒì°¨ ì‹œì‘"
                    })

                elif node_name == "finalizing":
                    yield send_event("agent", {
                        "agent": "pi",
                        "phase": "finalizing",
                        "message": "ğŸ‘” PI: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘..."
                    })

                # ìµœì¢… ìƒíƒœ ì €ì¥
                final_result = node_state

        # ì›Œí¬í”Œë¡œìš° ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        result = final_result if final_result else workflow.invoke(initial_state)

        # ì™„ë£Œ ì´ë²¤íŠ¸
        yield send_event("complete", {
            "message": "âœ… ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ",
            "report": result["final_report"],
            "iterations": result["iteration"],
            "messages": result["messages"]
        })

    except Exception as e:
        # ì—ëŸ¬ ì´ë²¤íŠ¸
        yield send_event("error", {
            "message": f"ì—ëŸ¬ ë°œìƒ: {str(e)}",
            "error": str(e)
        })


@app.post("/api/research/stream")
async def stream_research(request: ResearchRequest):
    """ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ë¥¼ SSEë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

    ì‹¤ì‹œê°„ìœ¼ë¡œ ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ì „ì†¡í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ íƒ€ì„ë¼ì¸ì„ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì´ë²¤íŠ¸ íƒ€ì…:
    - start: í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    - phase: ë‹¨ê³„ ë³€ê²½ (drafting, critique, finalizing)
    - agent: ì—ì´ì „íŠ¸ í™œë™ (scientist, critic, pi)
    - decision: Critic ê²°ì • (approve/revise)
    - iteration: ë°˜ë³µ íšŸìˆ˜ ë³€ê²½
    - complete: í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ
    - error: ì—ëŸ¬ ë°œìƒ
    """
    return StreamingResponse(
        generate_research_events(request.topic, request.constraints),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginx ë²„í¼ë§ ë°©ì§€
        }
    )


@app.post("/api/report/regenerate", response_model=RegenerateResponse)
def regenerate_section(request: RegenerateRequest):
    """ë³´ê³ ì„œ íŠ¹ì • ì„¹ì…˜ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.

    ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ í•´ë‹¹ ì„¹ì…˜ë§Œ ë‹¤ì‹œ ì‘ì„±í•©ë‹ˆë‹¤.
    Scientist ì—ì´ì „íŠ¸ê°€ ì›ë³¸ ë³´ê³ ì„œì™€ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì¬ì‘ì„±í•©ë‹ˆë‹¤.

    Args:
        request: ì„¹ì…˜ëª…, í”¼ë“œë°±, í˜„ì¬ ë³´ê³ ì„œ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ë³´ê³ ì„œ ì „ì²´
    """
    from agents.scientist import ScientistAgent
    from config import Config

    try:
        # Scientist ì—ì´ì „íŠ¸ ìƒì„±
        scientist = ScientistAgent(Config.OPENAI_MODEL)

        # ì¬ìƒì„± í”„ë¡¬í”„íŠ¸ ì‘ì„±
        prompt = f"""ë‹¤ìŒ ë³´ê³ ì„œì˜ '{request.section}' ì„¹ì…˜ì„ ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ê°œì„ í•˜ì„¸ìš”.

<í˜„ì¬ ë³´ê³ ì„œ>
{request.current_report}
</í˜„ì¬ ë³´ê³ ì„œ>

<ì‚¬ìš©ì í”¼ë“œë°±>
{request.feedback}
</ì‚¬ìš©ì í”¼ë“œë°±>

<ì§€ì¹¨>
1. '{request.section}' ì„¹ì…˜ë§Œ ì¬ì‘ì„±í•˜ì„¸ìš”
2. ì‚¬ìš©ì í”¼ë“œë°±ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì„¸ìš”
3. ë‹¤ë¥¸ ì„¹ì…˜ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”
4. ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”
5. ì „ì²´ ë³´ê³ ì„œ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”

<ê°œì„ ëœ ì „ì²´ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•˜ì„¸ìš”>
"""

        # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ê°„ë‹¨í•œ ì§ì ‘ í˜¸ì¶œ)
        # ì‹¤ì œë¡œëŠ” scientistì˜ LLMì„ ì§ì ‘ í˜¸ì¶œ
        from langchain_openai import ChatOpenAI
        from langchain_core.messages import HumanMessage

        llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            temperature=0.3,
        )

        response = llm.invoke([HumanMessage(content=prompt)])
        updated_report = response.content

        return RegenerateResponse(
            updated_report=updated_report,
            section=request.section,
            message=f"'{request.section}' ì„¹ì…˜ì´ ì¬ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to regenerate section: {str(e)}"
        )
