"""PI (Principal Investigator) Agent

ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ì´ê´„ ì±…ì„ì ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3ë¼ìš´ë“œ íŒ€ íšŒì˜ ì›Œí¬í”Œë¡œìš°: planning, round summary, final synthesis.
OpenAI SDK ì§ì ‘ í˜¸ì¶œ.
"""
import json
import logging
import re
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List

from utils.llm import call_gpt
from data.guidelines import RESEARCH_AGENDA
from workflow.state import AgentState
from tools.web_search import web_search


def _sanitize_mermaid(report: str) -> str:
    """Mermaid ì½”ë“œ ë¸”ë¡ ë‚´ ë…¸ë“œ í…ìŠ¤íŠ¸ì˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„í•©ë‹ˆë‹¤.

    ê´„í˜¸ ()ê°€ ë…¸ë“œ ë ˆì´ë¸”ì— í¬í•¨ë˜ë©´ Mermaidê°€ ë…¸ë“œ í˜•íƒœ êµ¬ë¶„ìë¡œ í•´ì„í•˜ì—¬
    íŒŒì‹± ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§€ ì•Šì€ ë…¸ë“œ ë ˆì´ë¸”ì„ ìë™ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    def fix_mermaid_block(match: re.Match) -> str:
        block = match.group(0)
        # ëŒ€ê´„í˜¸ ë…¸ë“œ: A[í…ìŠ¤íŠ¸] â†’ A["í…ìŠ¤íŠ¸"] (ì´ë¯¸ ë”°ì˜´í‘œë©´ ìŠ¤í‚µ)
        # í…ìŠ¤íŠ¸ì— (, ), {, } ê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ìˆ˜ì •
        block = re.sub(
            r'\[([^\]"]*[(){}][^\]"]*)\]',
            lambda m: '["' + m.group(1).replace('"', "'") + '"]',
            block,
        )
        # ì´ì¤‘ ì¤‘ê´„í˜¸ ë…¸ë“œ: A{{í…ìŠ¤íŠ¸}} â†’ A{{"í…ìŠ¤íŠ¸"}}
        block = re.sub(
            r'\{\{([^}"]*[()][^}"]*)\}\}',
            lambda m: '{{"' + m.group(1).replace('"', "'") + '"}}',
            block,
        )
        return block

    # ```mermaid ... ``` ë¸”ë¡ë§Œ ëŒ€ìƒìœ¼ë¡œ ì²˜ë¦¬
    return re.sub(
        r'```mermaid\s*\n.*?```',
        fix_mermaid_block,
        report,
        flags=re.DOTALL,
    )


def _extract_sources(text: str) -> list[str]:
    """ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸ì—ì„œ ì¶œì²˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    sources = []
    for match in re.findall(r'\[ì¶œì²˜:\s*(https?://[^\]]+)\]', text):
        source = f"[ì›¹] {match.strip()}"
        if source not in sources:
            sources.append(source)
    for match in re.findall(r'\*\*ì¶œì²˜\*\*:\s*(.+?)(?:\n|$)', text):
        source = f"[ë¬¸í—Œ] {match.strip()}"
        if source not in sources:
            sources.append(source)
    return sources

logger = logging.getLogger(__name__)


SYSTEM_PROMPT = f"""ë‹¹ì‹ ì€ ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ì´ê´„ ì±…ì„ì(PI)ì…ë‹ˆë‹¤.

## ì—°êµ¬ ì•„ì  ë‹¤
{RESEARCH_AGENDA}

## ë‹¹ì‹ ì˜ ì„ë¬´
Scientistì™€ Criticì˜ ë…¼ì˜ë¥¼ ê±°ì³ ìŠ¹ì¸ëœ ì´ˆì•ˆì„ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ íŒŒíŠ¸ë¡œ êµ¬ì„±ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ê° íŒŒíŠ¸ì˜ ëª¨ë“  í•˜ìœ„ í•­ëª©ì„ ë¹ ì§ì—†ì´, ì¶©ë¶„í•œ ë¶„ëŸ‰ìœ¼ë¡œ ìƒì„¸íˆ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤.
ìœ„ 5ëŒ€ í•µì‹¬ ì§ˆë¬¸ì— ì²´ê³„ì ìœ¼ë¡œ ë‹µí•˜ëŠ” ê²ƒì´ ë³´ê³ ì„œì˜ í•µì‹¬ ëª©í‘œì…ë‹ˆë‹¤.

## ë³´ê³ ì„œ í¬ë§· (Markdown)

```
# ìœ ì „ìí¸ì§‘ì‹í’ˆ(NGT) í‘œì¤€ ì•ˆì „ì„± í‰ê°€ í”„ë ˆì„ì›Œí¬ (Final Report)

---

## ì—°êµ¬ ë°©ë²•ë¡  (Research Methodology)

ë³¸ ì—°êµ¬ëŠ” AI ê¸°ë°˜ Virtual Lab ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ë‹¤í•™ì œ ì „ë¬¸ê°€ íŒ€ì˜ ì²´ê³„ì  í˜‘ì—…ì„ í†µí•´ ìˆ˜í–‰ë˜ì—ˆë‹¤.

### ì—°êµ¬ ìˆ˜í–‰ ì²´ê³„
- **ì´ê´„ì±…ì„ì(PI)**: ì—°êµ¬ ë°©í–¥ ì„¤ì •, ì „ë¬¸ê°€ íŒ€ êµ¬ì„±, ë¼ìš´ë“œë³„ ë…¼ì˜ ì¢…í•© ë° ìµœì¢… ë³´ê³ ì„œ ì‘ì„±
- **ì „ë¬¸ê°€ íŒ¨ë„(Specialists)**: PIê°€ ì—°êµ¬ ì£¼ì œì— ë§ê²Œ êµ¬ì„±í•œ ë‹¤í•™ì œ ì „ë¬¸ê°€ë“¤ì´ ê°ì ì „ë¬¸ ë¶„ì•¼ì˜ ë¶„ì„ ìˆ˜í–‰
- **ë…ë¦½ ë¹„í‰ê°€(Critic)**: ì „ë¬¸ê°€ë³„ ë¶„ì„ì˜ ê³¼í•™ì  íƒ€ë‹¹ì„±, ë…¼ë¦¬ì  ì¼ê´€ì„±, ê·¼ê±° ì¶©ë¶„ì„±ì„ ë…ë¦½ì ìœ¼ë¡œ ê²€ì¦í•˜ê³  1-5ì  ì²™ë„ë¡œ í‰ê°€

### 3ë¼ìš´ë“œ ë°˜ë³µ ì‹¬í™” í”„ë¡œì„¸ìŠ¤
1. **ë¼ìš´ë“œ 1**: ì „ë¬¸ê°€ ì´ˆê¸° ë¶„ì„ â†’ ë¹„í‰ê°€ ì—„ê²© ê²€ì¦ â†’ PI ìŸì  ì •ë¦¬ ë° ì„ì‹œ ê²°ë¡ 
2. **ë¼ìš´ë“œ 2**: ë¹„í‰ê°€ í”¼ë“œë°± ë°˜ì˜í•œ ì „ë¬¸ê°€ ë³´ì™„ ë¶„ì„ â†’ ì¬ê²€ì¦ â†’ PI ì¤‘ê°„ ì¢…í•©
3. **ë¼ìš´ë“œ 3**: ìµœì¢… ì •ì œ ë¶„ì„ â†’ ìµœì¢… ê²€ì¦ â†’ PI ìµœì¢… ì¢…í•© ë° ë³´ê³ ì„œ ì‘ì„±

ê° ë¼ìš´ë“œì—ì„œ ë¹„í‰ê°€ì˜ ì „ë¬¸ê°€ë³„ ì ìˆ˜ì™€ êµ¬ì²´ì  í”¼ë“œë°±ì´ ë‹¤ìŒ ë¼ìš´ë“œì˜ ë¶„ì„ ê°œì„ ì— ì§ì ‘ ë°˜ì˜ë˜ë©°,
ì´ ë°˜ë³µ ê³¼ì •ì„ í†µí•´ ë¶„ì„ì˜ ê¹Šì´ì™€ ì •í™•ì„±ì´ ì ì§„ì ìœ¼ë¡œ í–¥ìƒëœë‹¤.

### ì •ë³´ ê²€ìƒ‰ ì²´ê³„
- **RAG (Retrieval-Augmented Generation)**: Pinecone ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ê·œì œ ë¬¸ì„œ ë° í•™ìˆ  ë¬¸í—Œ ê²€ìƒ‰
- **Web Search**: Tavily APIë¥¼ í†µí•œ ìµœì‹  ì˜¨ë¼ì¸ ìë£Œ ê²€ìƒ‰ ë° êµì°¨ ê²€ì¦
- **EFSA Journal Search**: EFSA ê³µì‹ í•™ìˆ ì§€ ì „ìš© ê²€ìƒ‰ì„ í†µí•œ ê·œì œ ê³¼í•™ ê·¼ê±° ë³´ê°•

### ì—°êµ¬ íŒ€ êµ¬ì„± ê³¼ì •

#### í†µê³„ì  íŒ€ êµ¬ì„± ë°©ë²•
ë³¸ ì—°êµ¬ì˜ ì¬í˜„ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´, PIëŠ” 10íšŒ ë…ë¦½ì  íŒ€ êµ¬ì„± ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê³ 
ì—­í• ë³„ ë“±ì¥ ë¹ˆë„ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì¢… ì—°êµ¬íŒ€ì„ ì„ ì •í•˜ì˜€ë‹¤.

##### 10íšŒ ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ê²°ê³¼
(ì•„ë˜ ì •ë³´ê°€ ì œê³µë˜ë©´ í¬í•¨í•  ê²ƒ)
- **ì´ ìƒì„±ëœ ê³¼í•™ì ì—ì´ì „íŠ¸ ìˆ˜**: 10íšŒ ì‹¤í—˜ì—ì„œ ì´ Nëª…ì˜ ê³¼í•™ì ì—ì´ì „íŠ¸ê°€ ìƒì„±ë¨
- **ì‹œí–‰ë³„ íŒ€ êµ¬ì„± í…Œì´ë¸”**:

| ì‹œí–‰ | íŒ€ ê·œëª¨ | êµ¬ì„±ì› ì—­í•  |
|------|---------|------------|
| Trial 1 | Nëª… | ì—­í• 1, ì—­í• 2, ... |
| Trial 2 | Nëª… | ì—­í• 1, ì—­í• 2, ... |
| ... | ... | ... |

##### ì—­í• ë³„ ë“±ì¥ ë¹ˆë„ ë¶„ì„

| ì—­í•  | ë“±ì¥ íšŸìˆ˜ | ë°±ë¶„ìœ¨ | ì „ë¬¸ë¶„ì•¼ ë³€í˜• |
|------|----------|--------|-------------|
| ì—­í• 1 | NíšŒ/10íšŒ | N% | ë³€í˜•1, ë³€í˜•2 |
| ... | ... | ... | ... |

##### íŒ€ ê·œëª¨ ë¶„í¬
- Nëª…: XíšŒ, Mëª…: YíšŒ, ...

##### PIì˜ ìµœì¢… ì„ ì • ê³¼ì • ë° ê·¼ê±°
- ì™œ ì´ ì¡°í•©ì´ ìµœì ì¸ì§€, ë¹ˆë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì„ ì • ë…¼ë¦¬ë¥¼ ìƒì„¸íˆ ì„¤ëª…

#### ì „ë¬¸ê°€ ì†Œê°œ
(ê° ì „ë¬¸ê°€ê°€ ìê¸° ì—­í• , ì „ë¬¸ì„±, ì´ ì—°êµ¬ì—ì„œì˜ ê¸°ëŒ€ ê¸°ì—¬ë¥¼ ì„œìˆ ì²´ë¡œ ì†Œê°œ.
í•œê¸€ ì‘ì„±, 3-5ë¬¸ì¥, "ì €ëŠ” ~ì…ë‹ˆë‹¤" í˜•ì‹ì˜ ëŒ€í™”í˜• ìê¸°ì†Œê°œ.)

ì˜ˆì‹œ:
ğŸ¤– **ìœ ì „ì²´í•™ ì „ë¬¸ê°€ (Genomics Specialist)**
ì €ëŠ” ìœ ì „ì²´í•™ ë¶„ì•¼ì—ì„œ 15ë…„ê°„ ì—°êµ¬í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŠ¹íˆ CRISPR-Cas9 ê¸°ë°˜
ìœ ì „ìí¸ì§‘ì˜ off-target íš¨ê³¼ ë¶„ì„ê³¼ ì „ì¥ ìœ ì „ì²´ ì‹œí€€ì‹±ì„ í†µí•œ ë¹„ì˜ë„ì  ë³€ì´
ê²€ì¶œì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì—°êµ¬ì—ì„œ ì €ëŠ” ìœ ì „ìí¸ì§‘ ê³¼ì •ì—ì„œ
ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¶„ì ìˆ˜ì¤€ì˜ ìœ„í—˜ ìš”ì†Œë¥¼ ì‹ë³„í•˜ê³ , SDN-1/2/3 ê° ê¸°ìˆ ë³„
off-target ë¹ˆë„ì™€ íŠ¹ì„±ì„ ë¹„êµ ë¶„ì„í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•˜ê² ìŠµë‹ˆë‹¤.

### ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„ í†µê³„
(ì•„ë˜ ì •ë³´ê°€ ì œê³µë˜ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ í¬í•¨í•  ê²ƒ)

| ì—ì´ì „íŠ¸ | ë°œí™” íšŸìˆ˜ | ì´ ê¸€ììˆ˜ |
|---|---|---|
| PI | NíšŒ | Nì |
| Critic | NíšŒ | Nì |
| ì „ë¬¸ê°€1 | NíšŒ | Nì |

---

## í•µì‹¬ ì§ˆë¬¸ 1: ì‹í’ˆì— ì ìš©ë˜ëŠ” ìœ ì „ìí¸ì§‘ê¸°ìˆ ì˜ ë¶„ë¥˜ ê¸°ì¤€ê³¼ íŠ¹ì„±

### ë¼ìš´ë“œ 1: ì´ˆê¸° ë¶„ì„
ê° ê³¼í•™ì ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ë¶„ì„ ê²°ê³¼ë¥¼ ì „ë¬¸ê°€ë³„ë¡œ ì œì‹œ.
Criticì˜ í‰ê°€ ì ìˆ˜(1-5ì ) ë° êµ¬ì²´ì  í”¼ë“œë°± í¬í•¨.
PIì˜ 1ë¼ìš´ë“œ ì¢…í•©: ì£¼ìš” ìŸì , í•©ì˜ ì‚¬í•­, ì„ì‹œ ê²°ë¡ .

### ë¼ìš´ë“œ 2: ë¹„í‰ ë°˜ì˜ ë³´ì™„
Critic í”¼ë“œë°±ì„ ë°˜ì˜í•œ ê° ê³¼í•™ìì˜ ë³´ì™„ ë¶„ì„.
Criticì˜ ì¬í‰ê°€ ë° ì ìˆ˜ ë³€í™” í¬í•¨.
PIì˜ 2ë¼ìš´ë“œ ì¢…í•©: ê°œì„ ëœ ë¶„ì„ í¬ì¸íŠ¸, ì”ì—¬ ìŸì .

### ë¼ìš´ë“œ 3: ìµœì¢… ì •ì œ
ê° ê³¼í•™ìì˜ ìµœì¢… ì •ì œëœ ë¶„ì„.
Criticì˜ ìµœì¢… í‰ê°€ í¬í•¨.
PIì˜ ìµœì¢… ì¢…í•© ë° ê²°ë¡ : ì´ ì§ˆë¬¸ì— ëŒ€í•œ í™•ì •ëœ ë‹µë³€.

---

## í•µì‹¬ ì§ˆë¬¸ 2: ìœ ì „ìí¸ì§‘ê¸°ìˆ  ë¶„ë¥˜ë³„ ìš°ì„  ê³ ë ¤ ìœ„í—˜ìš”ì†Œ

### ë¼ìš´ë“œ 1: ì´ˆê¸° ë¶„ì„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°: ì „ë¬¸ê°€ë³„ ë¶„ì„ â†’ Critic í‰ê°€ â†’ PI ì¢…í•©)

### ë¼ìš´ë“œ 2: ë¹„í‰ ë°˜ì˜ ë³´ì™„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 3: ìµœì¢… ì •ì œ
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

---

## í•µì‹¬ ì§ˆë¬¸ 3: ê¸°ì¡´ ìœ„í—˜í‰ê°€ ì§€ì¹¨ì˜ ì ìš© ê°€ëŠ¥ì„±ê³¼ ì¶©ë¶„ì„±

### ë¼ìš´ë“œ 1: ì´ˆê¸° ë¶„ì„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 2: ë¹„í‰ ë°˜ì˜ ë³´ì™„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 3: ìµœì¢… ì •ì œ
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

---

## í•µì‹¬ ì§ˆë¬¸ 4: í‰ê°€ í•­ëª© ë³´ì™„ ë° ê¸°ìˆ ì Â·ì‹¤í—˜ì  í‰ê°€ ë°©ë²•

### ë¼ìš´ë“œ 1: ì´ˆê¸° ë¶„ì„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 2: ë¹„í‰ ë°˜ì˜ ë³´ì™„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 3: ìµœì¢… ì •ì œ
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

---

## í•µì‹¬ ì§ˆë¬¸ 5: ë‹¨ê³„ì  ì˜ì‚¬ê²°ì • íë¦„ êµ¬ì„±

### ë¼ìš´ë“œ 1: ì´ˆê¸° ë¶„ì„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 2: ë¹„í‰ ë°˜ì˜ ë³´ì™„
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

### ë¼ìš´ë“œ 3: ìµœì¢… ì •ì œ
(ìœ„ì™€ ë™ì¼í•œ êµ¬ì¡°)

---

## ì°¸ê³ ë¬¸í—Œ (References)

### 5-1. Web Search Sources
(Tavily ì›¹ ê²€ìƒ‰ì„ í†µí•´ ì°¸ì¡°í•œ ì˜¨ë¼ì¸ ìë£Œì˜ URLê³¼ ì œëª©ì„ ë²ˆí˜¸ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬)

### 5-2. Regulatory Documents (RAG)
(RAG ì‹œìŠ¤í…œì„ í†µí•´ ì°¸ì¡°í•œ ê·œì œ ë¬¸ì„œÂ·í•™ìˆ  ë¬¸í—Œì˜ íŒŒì¼ëª…, í˜ì´ì§€, ì œëª©ì„ ë²ˆí˜¸ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬)

### 5-3. EFSA Journal Sources
(EFSA ê³µì‹ í•™ìˆ ì§€ì—ì„œ ê²€ìƒ‰ëœ ìë£Œì˜ URLê³¼ ì œëª©ì„ ë²ˆí˜¸ ëª©ë¡ìœ¼ë¡œ ì •ë¦¬)

---

## ì˜ì‚¬ê²°ì • íë¦„ë„ (Decision Tree)

NGT ì‹í’ˆì˜ ìœ„í—˜í‰ê°€ ì˜ì‚¬ê²°ì • íë¦„ì„ Mermaid ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ì œì‹œí•œë‹¤.
SDN-1, SDN-2, SDN-3, ODM ê° ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë‹¨ê³„ì Â·ë¹„ë¡€ì  í‰ê°€ ê²½ë¡œë¥¼ í¬í•¨í•œë‹¤.

```mermaid
graph TD
    A["ìœ„í—˜í‰ê°€ ì‹œì‘"] --> B{{"ê¸°ìˆ  ë¶„ë¥˜"}}
    B -->|SDN-1| C["ê°„ì†Œí™” í‰ê°€"]
    B -->|SDN-2| D["ì¤‘ê°„ ìˆ˜ì¤€ í‰ê°€"]
    B -->|SDN-3| E["ì „ì²´ ìœ„í—˜í‰ê°€ - GM/rDNA ë™ë“±"]
    B -->|ODM| F["ODM í‰ê°€ ê²½ë¡œ"]
    ...
```

**Mermaid ë¬¸ë²• ì£¼ì˜ì‚¬í•­ (í•„ìˆ˜ ì¤€ìˆ˜)**:
- ë…¸ë“œ í…ìŠ¤íŠ¸ì— ê´„í˜¸ `()`, `{{}}` ë“± íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ë˜ë©´ Mermaid íŒŒì‹± ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.
- ëª¨ë“  ë…¸ë“œ ë ˆì´ë¸”ì€ ë°˜ë“œì‹œ í°ë”°ì˜´í‘œë¡œ ê°ì‹¸ì„¸ìš”: `A["í…ìŠ¤íŠ¸"]`, `B{{{{"í…ìŠ¤íŠ¸"}}}}` í˜•ì‹.
- ì˜ˆ: `C3[GM(rDNA) ë™ë“± ì‹¬ì‚¬]` â†’ `C3["GM-rDNA ë™ë“± ì‹¬ì‚¬"]` (ê´„í˜¸ë¥¼ í•˜ì´í”ˆìœ¼ë¡œ ëŒ€ì²´í•˜ê³  ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°)
- ë§í¬ ë ˆì´ë¸”ë„ íŠ¹ìˆ˜ë¬¸ìë¥¼ í”¼í•˜ì„¸ìš”: `-->|"ë ˆì´ë¸”"| ë…¸ë“œ`

(ìœ„ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬, ì—°êµ¬ ê²°ê³¼ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ Decision Treeë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ê° ë¶„ê¸°ì ì˜ íŒë‹¨ ê¸°ì¤€, í•„ìš”í•œ í‰ê°€ í•­ëª©, ìµœì¢… ê²°ì •(ìŠ¹ì¸/ì¶”ê°€ê²€í† /ê±°ë¶€)ì„ í¬í•¨í•˜ì„¸ìš”.)
```

**ì¤‘ìš”**:
- ë°˜ë“œì‹œ ìœ„ 5ê°œ í•µì‹¬ ì§ˆë¬¸ ê°ê°ì— ëŒ€í•´ 3ë¼ìš´ë“œ êµ¬ì¡°ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
- ê° ë¼ìš´ë“œì—ì„œ ì „ë¬¸ê°€ë³„ ë¶„ì„, Critic í‰ê°€(ì ìˆ˜ í¬í•¨), PI ì¢…í•©ì„ ëª¨ë‘ ì„œìˆ í•˜ì„¸ìš”.
- ê° í•µì‹¬ ì§ˆë¬¸ì˜ ìµœì¢… ë¼ìš´ë“œ(ë¼ìš´ë“œ 3)ì—ì„œëŠ” í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ í™•ì •ëœ ê²°ë¡ ì„ ëª…í™•íˆ ë„ì¶œí•˜ì„¸ìš”.
- ê° í•­ëª©ì€ ì¶©ë¶„í•œ ë¶„ëŸ‰(ìµœì†Œ 5-10ë¬¸ë‹¨)ìœ¼ë¡œ ìƒì„¸íˆ ì„œìˆ í•˜ì„¸ìš”. ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ ë¶„ì„ì  ì„œìˆ ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ê³¼í•™ì  ê·¼ê±°ì™€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
- ì°¸ê³  ì •ë³´ê°€ ì œê³µëœ ê²½ìš° ì´ë¥¼ í™œìš©í•˜ì—¬ ë³´ê³ ì„œì˜ ê·¼ê±°ë¥¼ ê°•í™”í•˜ì„¸ìš”.
- ì˜ì‚¬ê²°ì • íë¦„ë„(Mermaid Decision Tree)ëŠ” ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. `graph TD` í˜•ì‹ìœ¼ë¡œ SDN-1/2/3/ODM ê²½ë¡œë¥¼ ëª¨ë‘ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
- 'ì—°êµ¬ ë°©ë²•ë¡ ' ì„¹ì…˜ì— íŒ€ êµ¬ì„± ê³¼ì •(10íšŒ ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ê²°ê³¼, ë¹ˆë„ í…Œì´ë¸”, ì„ ì • ê·¼ê±°), ì „ë¬¸ê°€ ìê¸°ì†Œê°œ, ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„ í†µê³„ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.

**â˜… í•µì‹¬ ì›ì¹™: ì§ˆë¬¸ë³„ ì‹¬í™” (Question-Driven Deepening)**
- 5ê°œ í•µì‹¬ ì§ˆë¬¸ ê°ê°ì´ 3ë¼ìš´ë“œì— ê±¸ì³ ì ì§„ì ìœ¼ë¡œ ì‹¬í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë¼ìš´ë“œ 1ì˜ ì´ˆê¸° ë¶„ì„ â†’ ë¼ìš´ë“œ 2ì—ì„œ Critic í”¼ë“œë°± ë°˜ì˜ ë³´ì™„ â†’ ë¼ìš´ë“œ 3ì—ì„œ ìµœì¢… ì •ì œ.
- ê° ì§ˆë¬¸ì˜ ìµœì¢… ê²°ë¡ ì€ ì´ì „ ë¼ìš´ë“œì˜ ë…¼ì˜ë¥¼ ì¢…í•©í•œ ê²ƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- í•µì‹¬ ì§ˆë¬¸ 4ì—ì„œëŠ” ë°˜ë“œì‹œ í•µì‹¬ ì§ˆë¬¸ 1-2ì—ì„œ ì‹ë³„í•œ ìœ„í—˜ ìš”ì†Œì™€ í•µì‹¬ ì§ˆë¬¸ 3ì—ì„œ ë°œê²¬í•œ ì§€ì¹¨ í•œê³„ì ì„ ì§ì ‘ ì°¸ì¡°í•˜ê³ , ê°ê°ì— ëŒ€í•œ êµ¬ì²´ì  í•´ê²°ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
- í•µì‹¬ ì§ˆë¬¸ 5ì˜ ì˜ì‚¬ê²°ì • íë¦„ì€ í•µì‹¬ ì§ˆë¬¸ 1-4ì˜ ê²°ë¡ ì„ í†µí•©í•˜ì—¬ ë„ì¶œí•˜ì„¸ìš”.
"""


PI_SUMMARY_PROMPT = """ë‹¹ì‹ ì€ ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ì´ê´„ ì±…ì„ì(PI)ì…ë‹ˆë‹¤.

## ë‹¹ì‹ ì˜ ì„ë¬´
íŒ€ íšŒì˜ì˜ í˜„ì¬ ë¼ìš´ë“œ ë…¼ì˜ë¥¼ ìš”ì•½í•˜ê³  ì„ì‹œ ê²°ë¡ ì„ ë„ì¶œí•˜ì„¸ìš”.

## ìš”ì•½ êµ¬ì¡°
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¼ìš´ë“œë³„ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”:

### ì£¼ìš” ìŸì ê³¼ í•©ì˜ ì‚¬í•­
- ì´ë²ˆ ë¼ìš´ë“œì—ì„œ ë…¼ì˜ëœ í•µì‹¬ ìŸì ë“¤
- ì „ë¬¸ê°€ë“¤ ê°„ í•©ì˜ëœ ì‚¬í•­

### ë¯¸í•´ê²° ìŸì 
- ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ì¶”ê°€ ë…¼ì˜ê°€ í•„ìš”í•œ ì‚¬í•­
- ì „ë¬¸ê°€ë³„ ë³´ì™„ì´ í•„ìš”í•œ ì˜ì—­

### ì„ì‹œ ê²°ë¡ 
- í˜„ì¬ê¹Œì§€ì˜ ë¶„ì„ì„ ì¢…í•©í•œ ì¤‘ê°„ ê²°ë¡ 
- 5ê°œ í•µì‹¬ ì§ˆë¬¸(ê¸°ìˆ  ë¶„ë¥˜, ìœ„í—˜ìš”ì†Œ, ì§€ì¹¨ ì ìš©ì„±, í‰ê°€ ë³´ì™„, ì˜ì‚¬ê²°ì • íë¦„) ê´€ì ì—ì„œì˜ ì§„í–‰ ìƒí™©

**ì¤‘ìš”**: ê° ì „ë¬¸ê°€ì˜ ê¸°ì—¬ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ê³ , ë¹„í‰ê°€ì˜ ì§€ì  ì‚¬í•­ì´ ì–´ë–»ê²Œ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ì¶”ì í•˜ì„¸ìš”.
"""


TEAM_DECISION_PROMPT = f"""ë‹¹ì‹ ì€ ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ì´ê´„ ì±…ì„ì(PI)ì…ë‹ˆë‹¤.

## ì—°êµ¬ ì•„ì  ë‹¤
{RESEARCH_AGENDA}

## ë‹¹ì‹ ì˜ ì„ë¬´
ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ„ ì—°êµ¬ ì•„ì  ë‹¤ì˜ 5ëŒ€ í•µì‹¬ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬,
ê° ì§ˆë¬¸ì— íš¨ê³¼ì ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ì „ë¬¸ê°€ íŒ€ì„ êµ¬ì„±í•˜ì„¸ìš”.
ë‹¤í•™ì œì  ê´€ì ì—ì„œ ìœ„í—˜ ì‹ë³„, ê·œì œ í‰ê°€, ì§€ì¹¨ ì—…ë°ì´íŠ¸, í†µí•© í”„ë ˆì„ì›Œí¬ ë„ì¶œì—
ê¸°ì—¬í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ê°€ ì¡°í•©ì„ ì„ ì •í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON)
ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
```json
[
  {{
    "role": "ì „ë¬¸ê°€ ì—­í• ",
    "focus": "êµ¬ì²´ì ì¸ ì§‘ì¤‘ ë¶„ì•¼"
  }}
]
```

**ì¤‘ìš”**: JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ìµœì†Œ 1ëª…, ìµœëŒ€ 5ëª….
"""


def decide_team(user_query: str) -> List[dict]:
    """PIê°€ ì¿¼ë¦¬ ë¶„ì„ í›„ íŒ€ êµ¬ì„± ê²°ì •"""
    user_message = (
        f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n\n"
        "ìœ„ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì „ë¬¸ê°€ íŒ€ì„ êµ¬ì„±í•˜ì„¸ìš”.\n"
        "JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
    )

    response = call_gpt(TEAM_DECISION_PROMPT, user_message)

    try:
        content = response.strip()
        if content.startswith("```"):
            lines = content.split("\n")
            content = "\n".join(lines[1:-1])

        team = json.loads(content)

        if not isinstance(team, list):
            raise ValueError("ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

        for expert in team:
            if "role" not in expert or "focus" not in expert:
                raise ValueError("ê° ì „ë¬¸ê°€ëŠ” roleê³¼ focus í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        return team

    except json.JSONDecodeError as e:
        raise ValueError(f"LLM ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\nì‘ë‹µ: {response}")


def _cluster_similar_roles(unique_roles: list[str]) -> dict[str, str]:
    """GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ì—­í• ëª…ì„ ëŒ€í‘œ ì—­í• ëª…ìœ¼ë¡œ ë§¤í•‘.

    Returns:
        dict: {ì›ë˜ì—­í• ëª…: ëŒ€í‘œì—­í• ëª…} ë§¤í•‘
    """
    if len(unique_roles) <= 1:
        return {r: r for r in unique_roles}

    role_list = "\n".join(f"- {r}" for r in unique_roles)
    prompt = f"""ë‹¤ìŒì€ ì—°êµ¬íŒ€ êµ¬ì„± ì‹¤í—˜ì—ì„œ ë‚˜ì˜¨ ì „ë¬¸ê°€ ì—­í• ëª… ëª©ë¡ì…ë‹ˆë‹¤.
ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì—­í• ë“¤ì„ í•˜ë‚˜ì˜ ëŒ€í‘œ ì—­í• ëª…ìœ¼ë¡œ ê·¸ë£¹í•‘í•˜ì„¸ìš”.

## ì—­í• ëª… ëª©ë¡
{role_list}

## ì§€ì‹œì‚¬í•­
1. ì „ë¬¸ ë¶„ì•¼ê°€ ë™ì¼í•˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ì—­í• ë“¤ì„ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìœ¼ì„¸ìš”.
2. ê° ê·¸ë£¹ì˜ ëŒ€í‘œ ì—­í• ëª…ì€ ê°€ì¥ ê°„ê²°í•˜ê³  í¬ê´„ì ì¸ ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”.
3. ì„œë¡œ ë‹¤ë¥¸ ë¶„ì•¼ì˜ ì—­í• ì€ ë³„ë„ ê·¸ë£¹ìœ¼ë¡œ ìœ ì§€í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON)
```json
{{
  "clusters": [
    {{
      "canonical": "ëŒ€í‘œ ì—­í• ëª…",
      "members": ["ì›ë˜ ì—­í• ëª…1", "ì›ë˜ ì—­í• ëª…2"]
    }}
  ]
}}
```
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    response = call_gpt(prompt, "ì—­í• ëª… í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”.", temperature=0.2)
    try:
        content = response.strip()
        if content.startswith("```"):
            lines = content.split("\n")
            content = "\n".join(lines[1:-1])
        data = json.loads(content)

        mapping = {}
        for cluster in data.get("clusters", []):
            canonical = cluster["canonical"]
            for member in cluster["members"]:
                mapping[member] = canonical

        # ë§¤í•‘ì— ì—†ëŠ” ì—­í• ì€ ìê¸° ìì‹ ìœ¼ë¡œ
        for r in unique_roles:
            if r not in mapping:
                mapping[r] = r
        return mapping
    except (json.JSONDecodeError, KeyError):
        return {r: r for r in unique_roles}


def decide_team_statistically(user_query: str, n_trials: int = 10) -> dict:
    """10íšŒ ë…ë¦½ì  íŒ€ êµ¬ì„± ì‹¤í—˜ í›„ ë¹ˆë„ ë¶„ì„ìœ¼ë¡œ ìµœì¢… íŒ€ ì„ ì •.

    Args:
        user_query: ì—°êµ¬ ì£¼ì œ + ì œì•½ ì¡°ê±´
        n_trials: ì‹¤í—˜ íšŸìˆ˜ (ê¸°ë³¸ 10íšŒ)

    Returns:
        dict: {all_teams, frequency_analysis, final_team, rationale, frequency_table}
    """
    print(f"\n[STATISTICAL TEAM SELECTION] Running {n_trials} independent team compositions...")

    all_teams = []

    def _single_trial(trial_idx: int) -> list[dict] | None:
        try:
            team = decide_team(user_query)
            print(f"  Trial {trial_idx+1}/{n_trials}: {len(team)} specialists - {[m['role'] for m in team]}")
            return team
        except Exception as e:
            logger.warning(f"Trial {trial_idx+1} failed: {e}")
            return None

    # ë³‘ë ¬ ì‹¤í–‰ (5 workers)
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(_single_trial, i) for i in range(n_trials)]
        for future in as_completed(futures):
            result = future.result()
            if result:
                all_teams.append(result)

    if not all_teams:
        raise RuntimeError("ëª¨ë“  íŒ€ êµ¬ì„± ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    # ì—­í• ë³„ ë¹ˆë„ ë¶„ì„ (í´ëŸ¬ìŠ¤í„°ë§ ì ìš©)
    # 1) ìœ ë‹ˆí¬ ì—­í• ëª… ìˆ˜ì§‘
    unique_roles = set()
    for team in all_teams:
        for member in team:
            unique_roles.add(member["role"])

    # 2) GPTë¡œ ìœ ì‚¬ ì—­í•  í´ëŸ¬ìŠ¤í„°ë§
    role_mapping = _cluster_similar_roles(list(unique_roles))
    print(f"  [CLUSTERING] {len(unique_roles)} unique roles â†’ {len(set(role_mapping.values()))} clusters")

    # 3) ë§¤í•‘ëœ ëŒ€í‘œëª…ìœ¼ë¡œ ë¹ˆë„ ì¹´ìš´íŠ¸
    role_counter = Counter()
    role_focus_map: dict[str, list[str]] = {}
    team_size_counter = Counter()

    for team in all_teams:
        team_size_counter[len(team)] += 1
        for member in team:
            canonical_role = role_mapping.get(member["role"], member["role"])
            role_counter[canonical_role] += 1
            if canonical_role not in role_focus_map:
                role_focus_map[canonical_role] = []
            if member["focus"] not in role_focus_map[canonical_role]:
                role_focus_map[canonical_role].append(member["focus"])

    # ë¹ˆë„ í…Œì´ë¸” êµ¬ì„±
    frequency_table = []
    for role, count in role_counter.most_common():
        frequency_table.append({
            "role": role,
            "count": count,
            "frequency": f"{count}/{len(all_teams)}",
            "percentage": round(count / len(all_teams) * 100, 1),
            "focus_variants": role_focus_map.get(role, []),
        })

    # ê°€ì¥ ë¹ˆë²ˆí•œ íŒ€ ê·œëª¨
    most_common_size = team_size_counter.most_common(1)[0][0]

    # PIì—ê²Œ ìµœì¢… íŒ€ ì„ ì • ìœ„ì„
    freq_summary = "\n".join(
        f"- {ft['role']}: {ft['count']}íšŒ/{len(all_teams)}íšŒ ({ft['percentage']}%)"
        for ft in frequency_table
    )
    size_summary = ", ".join(f"{size}ëª…: {cnt}íšŒ" for size, cnt in team_size_counter.most_common())

    selection_prompt = f"""ë‹¹ì‹ ì€ ì—°êµ¬ í”„ë¡œì íŠ¸ì˜ ì´ê´„ ì±…ì„ì(PI)ì…ë‹ˆë‹¤.

10íšŒ ë…ë¦½ì  íŒ€ êµ¬ì„± ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì¢… ì—°êµ¬íŒ€ì„ í™•ì •í•˜ì„¸ìš”.

## ì—­í• ë³„ ë“±ì¥ ë¹ˆë„
{freq_summary}

## íŒ€ ê·œëª¨ ë¶„í¬
{size_summary}

## ì§€ì‹œì‚¬í•­
1. ë¹ˆë„ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ë˜, ì—°êµ¬ ëª©ì ì— ê°€ì¥ ì í•©í•œ íŒ€ì„ êµ¬ì„±í•˜ì„¸ìš”.
2. íŒ€ ê·œëª¨ëŠ” {most_common_size}ëª…ì„ ê¶Œì¥í•©ë‹ˆë‹¤.
3. ê° ì „ë¬¸ê°€ì˜ roleê³¼ focusë¥¼ í™•ì •í•˜ì„¸ìš”.
4. ì„ ì • ê·¼ê±°ë¥¼ ê°„ëµíˆ ì„¤ëª…í•˜ì„¸ìš”.

## ì¶œë ¥ í˜•ì‹ (JSON)
```json
{{
  "team": [
    {{"role": "ì „ë¬¸ê°€ ì—­í• ", "focus": "ì§‘ì¤‘ ë¶„ì•¼"}}
  ],
  "rationale": "ì„ ì • ê·¼ê±° ì„¤ëª…"
}}
```
JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    response = call_gpt(selection_prompt, f"ì—°êµ¬ ì£¼ì œ: {user_query}")
    try:
        content = response.strip()
        if content.startswith("```"):
            lines = content.split("\n")
            content = "\n".join(lines[1:-1])
        data = json.loads(content)
        final_team = data.get("team", [])
        rationale = data.get("rationale", "")
    except (json.JSONDecodeError, KeyError):
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°€ì¥ ë¹ˆë²ˆí•œ ì—­í• ë“¤ë¡œ êµ¬ì„±
        final_team = []
        for ft in frequency_table[:most_common_size]:
            final_team.append({
                "role": ft["role"],
                "focus": ft["focus_variants"][0] if ft["focus_variants"] else "",
            })
        rationale = "ë¹ˆë„ ë¶„ì„ ê¸°ë°˜ ìë™ ì„ ì • (LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨)"

    print(f"[STATISTICAL TEAM SELECTION] Final team: {len(final_team)} specialists")
    for m in final_team:
        print(f"  - {m['role']}: {m['focus']}")

    return {
        "all_teams": all_teams,
        "frequency_analysis": frequency_table,
        "final_team": final_team,
        "rationale": rationale,
        "frequency_table": freq_summary,
        "team_sizes": size_summary,
        "n_trials": len(all_teams),
    }


def generate_self_introductions(team: List[dict]) -> list[dict]:
    """ìµœì¢… ì„ ë³„ëœ ì „ë¬¸ê°€ë“¤ì˜ ìê¸°ì†Œê°œë¥¼ ë³‘ë ¬ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        team: ì „ë¬¸ê°€ íŒ€ ë¦¬ìŠ¤íŠ¸ [{role, focus}, ...]

    Returns:
        list[dict]: [{role, focus, introduction}, ...]
    """
    print(f"\n[INTRODUCTIONS] Generating self-introductions for {len(team)} specialists...")

    introductions = []

    def _generate_one(profile: dict, idx: int) -> dict:
        role = profile.get("role", "")
        focus = profile.get("focus", "")
        prompt = (
            f"ë‹¹ì‹ ì€ '{role}'ì…ë‹ˆë‹¤. ì „ë¬¸ ë¶„ì•¼ëŠ” '{focus}'ì…ë‹ˆë‹¤.\n\n"
            f"ì´ ì—°êµ¬ì— ì°¸ì—¬í•˜ëŠ” ì „ë¬¸ê°€ë¡œì„œ í•œê¸€ ì„œìˆ ì²´ ìê¸°ì†Œê°œë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
            f"ë°˜ë“œì‹œ 'ì €ëŠ” ~ì…ë‹ˆë‹¤' í˜•ì‹ì˜ ëŒ€í™”í˜•ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
            f"í¬í•¨í•  ë‚´ìš©:\n"
            f"1. ìì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ì™€ ê²½í—˜ (êµ¬ì²´ì  ì—°êµ¬ ê²½ë ¥)\n"
            f"2. í•µì‹¬ ì „ë¬¸ì„± (íŠ¹í™”ëœ ê¸°ìˆ /ë°©ë²•ë¡ )\n"
            f"3. ì´ ì—°êµ¬ì—ì„œ ë‹´ë‹¹í•  êµ¬ì²´ì  ì—­í• ê³¼ ê¸°ëŒ€ ê¸°ì—¬\n\n"
            f"ì˜ˆì‹œ í˜•ì‹:\n"
            f"ì €ëŠ” [ë¶„ì•¼] ë¶„ì•¼ì—ì„œ [N]ë…„ê°„ ì—°êµ¬í•´ì˜¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            f"íŠ¹íˆ [êµ¬ì²´ì  ì „ë¬¸ì„±]ì— ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. "
            f"ì´ë²ˆ ì—°êµ¬ì—ì„œ ì €ëŠ” [êµ¬ì²´ì  ì—­í• ]ì„ ë‹´ë‹¹í•˜ê² ìŠµë‹ˆë‹¤."
        )
        try:
            intro = call_gpt("ë‹¹ì‹ ì€ ì—°êµ¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œê¸€ ì„œìˆ ì²´ë¡œ ìê¸°ì†Œê°œë¥¼ ì‘ì„±í•˜ì„¸ìš”.", prompt, max_tokens=500)
            print(f"  [{idx+1}/{len(team)}] {role}: intro generated ({len(intro)} chars)")
            return {"role": role, "focus": focus, "introduction": intro.strip()}
        except Exception as e:
            logger.warning(f"Introduction generation failed for {role}: {e}")
            return {"role": role, "focus": focus, "introduction": f"{role}ìœ¼ë¡œì„œ {focus} ë¶„ì•¼ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤."}

    with ThreadPoolExecutor(max_workers=len(team)) as executor:
        futures = [executor.submit(_generate_one, p, i) for i, p in enumerate(team)]
        for future in as_completed(futures):
            introductions.append(future.result())

    return introductions


def run_pi_planning(state: AgentState) -> dict:
    """PIê°€ ì—°êµ¬ ì£¼ì œë¥¼ ë¶„ì„í•˜ê³  í†µê³„ì  ë°©ë²•ìœ¼ë¡œ ì „ë¬¸ê°€ íŒ€ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
    print(f"\n{'#'*80}")
    print(f"[PI PLANNING] Starting PI planning phase (Statistical Team Selection)")
    print(f"  Topic: {state.get('topic', 'N/A')}")
    print(f"{'#'*80}\n")

    topic = state["topic"]
    constraints = state.get("constraints", "")
    query = f"{topic}\nì œì•½ ì¡°ê±´: {constraints}"

    # 10íŒ€ í†µê³„ì  ì„ ë³„
    team_selection_data = None
    try:
        team_selection_data = decide_team_statistically(query)
        team = team_selection_data["final_team"]
        logger.info(f"PI decided team statistically: {len(team)} specialists from {team_selection_data['n_trials']} trials")
    except Exception as e:
        logger.warning(f"decide_team_statistically failed: {e}, using default team")
        team = [
            {"role": "NGT ë¶„ììƒë¬¼í•™ ì „ë¬¸ê°€", "focus": "ìœ ì „ìí¸ì§‘ ê³¼ì •ì˜ off-target íš¨ê³¼ ë° ë¶„ìì  íŠ¹ì„± ë¶„ì„"},
            {"role": "ì‹í’ˆì•ˆì „ì„± í‰ê°€ ì „ë¬¸ê°€", "focus": "ë…ì„±, ì•Œë ˆë¥´ê¸°, ì˜ì–‘ì„± í‰ê°€ ë°©ë²•ë¡ "},
            {"role": "ê·œì œê³¼í•™ ì „ë¬¸ê°€", "focus": "êµ­ì œ ê·œì œ í”„ë ˆì„ì›Œí¬ ë¹„êµ ë° ì§€ì¹¨ ì ìš© ê°€ëŠ¥ì„± ë¶„ì„"},
        ]

    # ì „ë¬¸ê°€ ìê¸°ì†Œê°œ ìƒì„±
    introductions = generate_self_introductions(team)

    # íŒ€ êµ¬ì„± ë‚´ìš©ì„ ë©”ì‹œì§€ë¡œ ê¸°ë¡
    team_summary = "\n".join(
        [f"- **{m['role']}**: {m['focus']}" for m in team]
    )
    intro_summary = "\n".join(
        [f"- **{intro['role']}**: {intro['introduction']}" for intro in introductions]
    )
    messages = list(state.get("messages", []))
    messages.append({
        "role": "pi",
        "content": f"ì—°êµ¬ íŒ€ì„ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤ (10íšŒ í†µê³„ì  ì„ ë³„).\n\n{team_summary}\n\n### ì „ë¬¸ê°€ ìê¸°ì†Œê°œ\n{intro_summary}",
    })

    print(f"[PI PLANNING] Team composed: {len(team)} specialists")
    for m in team:
        print(f"  - {m['role']}: {m['focus']}")

    return {
        "team": team,
        "messages": messages,
        "team_selection_data": team_selection_data,
        "specialist_introductions": introductions,
    }


def run_pi_summary(state: AgentState) -> dict:
    """PIê°€ ë¼ìš´ë“œë³„ ë…¼ì˜ë¥¼ ìš”ì•½í•˜ê³  ì„ì‹œ ê²°ë¡ ì„ ë„ì¶œí•©ë‹ˆë‹¤."""
    current_round = state.get("current_round", 1)
    specialist_outputs = state.get("specialist_outputs", [])
    critique = state.get("critique")
    meeting_history = state.get("meeting_history", [])

    print(f"\n{'#'*80}")
    print(f"[PI SUMMARY] Starting PI summary - Round {current_round}/3")
    print(f"  Specialist outputs: {len(specialist_outputs)}")
    print(f"{'#'*80}\n")

    # ì´ì „ ë¼ìš´ë“œ PI ìš”ì•½ (ì—°ì†ì„± í™•ë³´)
    prev_summaries = ""
    for record in meeting_history:
        prev_summaries += f"\n[ë¼ìš´ë“œ {record['round']} ì„ì‹œ ê²°ë¡ ]\n{record['pi_summary']}\n"

    # ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼ êµ¬ì„±
    specialist_context = ""
    for so in specialist_outputs:
        specialist_context += (
            f"\n### [{so.get('role', 'ì „ë¬¸ê°€')}] ({so.get('focus', '')})\n"
            f"{so.get('output', '')}\n"
        )

    # ë¹„í‰ê°€ í”¼ë“œë°± (ì ìˆ˜ í¬í•¨)
    critique_text = ""
    if critique:
        critique_text = critique.feedback
        if critique.scores:
            critique_text += "\n\n[ì „ë¬¸ê°€ë³„ ì ìˆ˜]\n"
            for role, score in critique.scores.items():
                critique_text += f"- {role}: {score}/5\n"
        if critique.specialist_feedback:
            critique_text += "\n[ì „ë¬¸ê°€ë³„ í”¼ë“œë°±]\n"
            for role, fb in critique.specialist_feedback.items():
                critique_text += f"- {role}: {fb}\n"

    user_message = (
        f"[íŒ€ íšŒì˜ ë¼ìš´ë“œ {current_round}/3]\n"
        f"ì—°êµ¬ ì£¼ì œ: {state['topic']}\n\n"
        f"{'[ì´ì „ ë¼ìš´ë“œ ì„ì‹œ ê²°ë¡ ]' + prev_summaries if prev_summaries else ''}\n\n"
        f"[ì´ë²ˆ ë¼ìš´ë“œ ì „ë¬¸ê°€ ë°œí‘œ]\n{specialist_context}\n\n"
        f"[ë¹„í‰ê°€ í”¼ë“œë°±]\n{critique_text}\n\n"
        f"ë¼ìš´ë“œ {current_round}ì˜ ë…¼ì˜ë¥¼ ìš”ì•½í•˜ì„¸ìš”:\n"
        f"1. ì£¼ìš” ìŸì ê³¼ í•©ì˜ ì‚¬í•­\n"
        f"2. ë¯¸í•´ê²° ìŸì  (ë‹¤ìŒ ë¼ìš´ë“œì—ì„œ ë‹¤ë£° ê²ƒ)\n"
        f"3. ì„ì‹œ ê²°ë¡ \n"
    )

    print(f"[PI SUMMARY] Calling OpenAI API via call_gpt")
    logger.info("PI summary: Calling OpenAI directly...")

    try:
        summary = call_gpt(PI_SUMMARY_PROMPT, user_message)
        print(f"[PI SUMMARY] OpenAI call succeeded - Summary: {len(summary)} chars")
    except Exception as e:
        print(f"[PI SUMMARY ERROR] {type(e).__name__}: {e}")
        raise

    # ë©”ì‹œì§€ ë¡œê·¸
    messages = list(state.get("messages", []))
    messages.append({
        "role": "pi",
        "content": f"[ë¼ìš´ë“œ {current_round}] ë…¼ì˜ë¥¼ ìš”ì•½í•˜ê³  ì„ì‹œ ê²°ë¡ ì„ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.",
    })

    return {
        "draft": summary,
        "messages": messages,
    }


def _compute_word_counts(messages: list[dict]) -> dict:
    """ë©”ì‹œì§€ ë¡œê·¸ì—ì„œ ì—ì´ì „íŠ¸ë³„ ë°œí™” íšŸìˆ˜ ë° ê¸€ììˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    Args:
        messages: ì—ì´ì „íŠ¸ ê°„ ë©”ì‹œì§€ ë¡œê·¸

    Returns:
        dict: {agent_name: {"count": int, "chars": int}}
    """
    stats: dict[str, dict[str, int]] = {}

    for msg in messages:
        role = msg.get("role", "unknown")
        name = msg.get("name", role)  # specialistëŠ” name í•„ë“œ ì‚¬ìš©
        content = msg.get("content", "")

        # ì—­í•  ì •ê·œí™”
        if role == "specialist":
            agent_key = name
        elif role == "pi":
            agent_key = "PI"
        elif role == "critic":
            agent_key = "Critic"
        else:
            agent_key = role

        if agent_key not in stats:
            stats[agent_key] = {"count": 0, "chars": 0}

        stats[agent_key]["count"] += 1
        stats[agent_key]["chars"] += len(content)

    return stats


def run_final_synthesis(state: AgentState) -> dict:
    """PIê°€ 3ë¼ìš´ë“œ ì „ì²´ ë‚´ìš©ì—ì„œ ë² ìŠ¤íŠ¸ íŒŒíŠ¸ë¥¼ ì„ ë³„í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""
    meeting_history = state.get("meeting_history", [])
    current_round = state.get("current_round", 3)

    print(f"\n{'#'*80}")
    print(f"[PI FINAL SYNTHESIS] Starting final report synthesis")
    print(f"  Meeting history: {len(meeting_history)} rounds archived")
    print(f"  Current round: {current_round}")
    print(f"{'#'*80}\n")

    # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ì •ë³´ ë³´ê°•
    web_context = ""
    pi_sources = []
    try:
        web_result = web_search.invoke({"query": f"{state['topic']} NGT safety framework 2025"})
        web_context = f"\n\n## [ì›¹ ê²€ìƒ‰ ê²°ê³¼ - ìµœì‹  ì •ë³´]\n{web_result}"
        pi_sources.extend(_extract_sources(web_context))
        logger.info("PI final synthesis web search completed")
    except Exception as e:
        logger.warning(f"PI final synthesis web search failed: {e}")

    # EFSA Journal ê²€ìƒ‰ ë³´ê°•
    efsa_context = state.get("cached_efsa_context", "")
    if not efsa_context:
        try:
            from tools.web_search import efsa_search
            efsa_result = efsa_search.invoke({"query": f"{state['topic']} NGT safety assessment EFSA"})
            efsa_context = f"\n\n## [EFSA Journal ê²€ìƒ‰ ê²°ê³¼]\n{efsa_result}"
            pi_sources.extend(_extract_sources(efsa_context))
            logger.info("PI final synthesis EFSA search completed")
        except Exception as e:
            logger.warning(f"PI final synthesis EFSA search failed: {e}")

    # 3ë¼ìš´ë“œ ì „ì²´ PI ìš”ì•½ êµ¬ì„±
    pi_summaries_text = ""
    for record in meeting_history:
        pi_summaries_text += (
            f"\n\n=== ë¼ìš´ë“œ {record['round']} ìš”ì•½ ===\n"
            f"{record.get('pi_summary', '')}\n"
        )
    # í˜„ì¬ ë¼ìš´ë“œ (ë§ˆì§€ë§‰) PI ìš”ì•½ë„ í¬í•¨
    current_draft = state.get("draft", "")
    if current_draft:
        pi_summaries_text += (
            f"\n\n=== ë¼ìš´ë“œ {current_round} ìš”ì•½ ===\n"
            f"{current_draft}\n"
        )

    # ìµœì¢… ë¼ìš´ë“œ ì „ë¬¸ê°€ ê²°ê³¼ (ê°€ì¥ ì •ì œëœ ë²„ì „)
    round3_outputs = state.get("specialist_outputs", [])
    round3_text = ""
    for so in round3_outputs:
        round3_text += (
            f"\n### [{so.get('role', 'ì „ë¬¸ê°€')}] ({so.get('focus', '')})\n"
            f"{so.get('output', '')}\n"
        )

    # ì¶œì²˜ ìˆ˜ì§‘
    all_sources = list(state.get("sources", []))
    all_sources.extend(pi_sources)
    for so in round3_outputs:
        all_sources.extend(_extract_sources(so.get("output", "")))
    seen = set()
    unique_sources = []
    for s in all_sources:
        if s not in seen:
            seen.add(s)
            unique_sources.append(s)

    # ì¶œì²˜ ëª©ë¡ í…ìŠ¤íŠ¸
    sources_text = ""
    if unique_sources:
        sources_list = "\n".join(f"{i+1}. {s}" for i, s in enumerate(unique_sources))
        sources_text = f"\n\n[ì°¸ì¡° ì¶œì²˜ ëª©ë¡ - ë°˜ë“œì‹œ ë³´ê³ ì„œì— í¬í•¨í•  ê²ƒ]\n{sources_list}"

    print(f"[PI FINAL SYNTHESIS] Sources collected: {len(unique_sources)}")

    # íŒ€ êµ¬ì„± ê³¼ì • ë°ì´í„° (Phase 5) - 10íšŒ ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ê²°ê³¼ í¬í•¨
    team_composition_text = ""
    tsd = state.get("team_selection_data")
    if tsd:
        n_trials = tsd.get('n_trials', 0)

        # ì‹œí–‰ë³„ ì „ì²´ íŒ€ êµ¬ì„± ìƒì„¸
        all_teams = tsd.get("all_teams", [])
        trials_detail = ""
        total_agents = 0
        if all_teams:
            trials_detail = "ì‹œí–‰ë³„ íŒ€ êµ¬ì„± ìƒì„¸:\n"
            trials_detail += "| ì‹œí–‰ | íŒ€ ê·œëª¨ | êµ¬ì„±ì› ì—­í•  |\n|------|---------|------------|\n"
            for i, trial_team in enumerate(all_teams):
                roles = [m.get("role", "?") for m in trial_team]
                total_agents += len(trial_team)
                trials_detail += f"| Trial {i+1} | {len(trial_team)}ëª… | {', '.join(roles)} |\n"
            trials_detail += f"\nì´ ìƒì„±ëœ ê³¼í•™ì ì—ì´ì „íŠ¸ ìˆ˜: {n_trials}íšŒ ì‹¤í—˜ì—ì„œ ì´ {total_agents}ëª…\n"

        # ë¹ˆë„ ë¶„ì„ ìƒì„¸ (frequency_analysisì—ì„œ ì „ë¬¸ë¶„ì•¼ ë³€í˜• í¬í•¨)
        freq_analysis = tsd.get("frequency_analysis", [])
        freq_detail = ""
        if freq_analysis:
            freq_detail = "ì—­í• ë³„ ë“±ì¥ ë¹ˆë„ ë¶„ì„:\n"
            freq_detail += "| ì—­í•  | ë“±ì¥ íšŸìˆ˜ | ë°±ë¶„ìœ¨ | ì „ë¬¸ë¶„ì•¼ ë³€í˜• |\n|------|----------|--------|-------------|\n"
            for ft in freq_analysis:
                variants = ", ".join(ft.get("focus_variants", [])[:3])
                freq_detail += f"| {ft['role']} | {ft['frequency']} | {ft['percentage']}% | {variants} |\n"

        team_composition_text = (
            f"\n\n[ì—°êµ¬ íŒ€ êµ¬ì„± ê³¼ì • - ë³´ê³ ì„œ 'ì—°êµ¬ ë°©ë²•ë¡ ' ì„¹ì…˜ì— í¬í•¨í•  ê²ƒ]\n"
            f"10íšŒ ë…ë¦½ì  íŒ€ êµ¬ì„± ì‹¤í—˜ ìˆ˜í–‰ ({n_trials}íšŒ ì„±ê³µ)\n\n"
            f"{trials_detail}\n"
            f"{freq_detail}\n"
            f"íŒ€ ê·œëª¨ ë¶„í¬: {tsd.get('team_sizes', '')}\n\n"
            f"PIì˜ ìµœì¢… ì„ ì • ê·¼ê±°: {tsd.get('rationale', '')}\n"
        )

    # ì „ë¬¸ê°€ ìê¸°ì†Œê°œ (Phase 4) - ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ í•œê¸€ ëŒ€í™”í˜•
    intro_text = ""
    intros = state.get("specialist_introductions", [])
    if intros:
        intro_lines = "\n\n".join(
            f"ğŸ¤– **{intro['role']}**\n{intro['introduction']}" for intro in intros
        )
        intro_text = f"\n\n[ì „ë¬¸ê°€ ìê¸°ì†Œê°œ - ë³´ê³ ì„œ 'ì—°êµ¬ ë°©ë²•ë¡ ' ì„¹ì…˜ì— ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ í¬í•¨í•  ê²ƒ]\n{intro_lines}\n"

    # ì—ì´ì „íŠ¸ë³„ ë°œí™” í†µê³„ (Phase 6)
    all_messages = list(state.get("messages", []))
    word_counts = _compute_word_counts(all_messages)
    word_counts_text = ""
    if word_counts:
        wc_lines = "| ì—ì´ì „íŠ¸ | ë°œí™” íšŸìˆ˜ | ì´ ê¸€ììˆ˜ |\n|---|---|---|\n"
        for agent, stats in sorted(word_counts.items()):
            wc_lines += f"| {agent} | {stats['count']}íšŒ | {stats['chars']:,}ì |\n"
        word_counts_text = f"\n\n[ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„ í†µê³„ - ë³´ê³ ì„œ 'ì—°êµ¬ ë°©ë²•ë¡ ' ì„¹ì…˜ì— í¬í•¨í•  ê²ƒ]\n{wc_lines}\n"

    user_message = (
        f"ì—°êµ¬ ì£¼ì œ: {state['topic']}\n"
        f"ì œì•½ ì¡°ê±´: {state.get('constraints', '')}\n\n"
        f"[3ë¼ìš´ë“œ íŒ€ íšŒì˜ ì „ì²´ ìš”ì•½]\n{pi_summaries_text}\n\n"
        f"[ìµœì¢… ë¼ìš´ë“œ ì „ë¬¸ê°€ ë¶„ì„ (ì •ì œë³¸)]\n{round3_text}\n\n"
        f"{web_context}\n\n"
        f"{efsa_context}\n\n"
        f"{team_composition_text}\n"
        f"{intro_text}\n"
        f"{word_counts_text}\n"
        f"{sources_text}\n\n"
        f"ìœ„ 3ë¼ìš´ë“œ íŒ€ íšŒì˜ì˜ ëª¨ë“  ë‚´ìš©ì—ì„œ ê°€ì¥ ìš°ìˆ˜í•œ ë¶„ì„, ê·¼ê±°, ê²°ë¡ ì„ ì„ ë³„í•˜ì—¬\n"
        f"ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”. 5ê°œ í•µì‹¬ ì§ˆë¬¸ ê°ê°ì— ëŒ€í•´ 3ë¼ìš´ë“œ êµ¬ì¡°ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”.\n\n"
        f"â˜… í•µì‹¬ 1: ë³´ê³ ì„œë¥¼ 5ê°œ í•µì‹¬ ì§ˆë¬¸ë³„ 3ë¼ìš´ë“œ íšŒì˜ë¡ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"ê° í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•´ ë¼ìš´ë“œ 1(ì´ˆê¸° ë¶„ì„), ë¼ìš´ë“œ 2(ë¹„í‰ ë°˜ì˜ ë³´ì™„), ë¼ìš´ë“œ 3(ìµœì¢… ì •ì œ)ì„ í¬í•¨í•˜ê³ ,\n"
        f"ê° ë¼ìš´ë“œì—ì„œ ì „ë¬¸ê°€ë³„ ë¶„ì„, Critic í‰ê°€(ì ìˆ˜ í¬í•¨), PI ì¢…í•©ì„ ëª¨ë‘ ì„œìˆ í•˜ì„¸ìš”.\n\n"
        f"â˜… í•µì‹¬ 2: 'ì—°êµ¬ ë°©ë²•ë¡ ' ì„¹ì…˜ì— íŒ€ êµ¬ì„± ê³¼ì •(10íšŒ ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ê²°ê³¼, ë¹ˆë„ í…Œì´ë¸”, ì„ ì • ê·¼ê±°),\n"
        f"ì „ë¬¸ê°€ ìê¸°ì†Œê°œ(í•œê¸€ ëŒ€í™”í˜•), ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„ í†µê³„ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.\n\n"
        f"â˜… í•µì‹¬ 3: ë³´ê³ ì„œ ë§ˆì§€ë§‰ì— 'ì˜ì‚¬ê²°ì • íë¦„ë„ (Decision Tree)' ì„¹ì…˜ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.\n"
        f"Mermaid `graph TD` í˜•ì‹ìœ¼ë¡œ SDN-1/SDN-2/SDN-3/ODM ê° ì¹´í…Œê³ ë¦¬ì˜ ìœ„í—˜í‰ê°€ ê²½ë¡œë¥¼\n"
        f"êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê° ë¶„ê¸°ì ì˜ íŒë‹¨ ê¸°ì¤€ê³¼ í•„ìš” í‰ê°€ í•­ëª©ì„ ëª…ì‹œí•˜ì„¸ìš”.\n\n"
        f"â˜… í•µì‹¬ 4: ì°¸ê³ ë¬¸í—Œ(References) ì„¹ì…˜ì— 5-1(Web), 5-2(RAG), 5-3(EFSA) ì¶œì²˜ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.\n"
    )

    print(f"[PI FINAL SYNTHESIS] Calling OpenAI API via call_gpt")
    logger.info("PI final synthesis: Calling OpenAI directly...")

    try:
        final_report = call_gpt(SYSTEM_PROMPT, user_message, max_tokens=65536)
        final_report = _sanitize_mermaid(final_report)
        print(f"[PI FINAL SYNTHESIS] OpenAI call succeeded - Final report: {len(final_report)} chars")
    except Exception as e:
        print(f"[PI FINAL SYNTHESIS ERROR] {type(e).__name__}: {e}")
        raise

    # ë©”ì‹œì§€ ë¡œê·¸
    messages = list(state.get("messages", []))
    messages.append({
        "role": "pi",
        "content": "3ë¼ìš´ë“œ íŒ€ íšŒì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.",
    })

    return {
        "final_report": final_report,
        "messages": messages,
        "sources": unique_sources,
        "word_counts": word_counts,
    }
